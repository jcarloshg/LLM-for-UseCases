# Weekend LLMOps Project: User Story to Test Cases Generator

## üéØ Project Goal

Build an **automated test case generator** that converts user stories into structured test cases using LLMs, validates quality with MLflow, and serves via FastAPI.

---

## üìã Project Overview

**What You'll Build:**

- REST API that accepts user stories
- LLM generates structured test cases (Given-When-Then format)
- Quality validation using multiple metrics
- MLflow tracking for experiments and outputs
- Complete evaluation framework

**Example Flow:**

```
User Story Input:
"As a user, I want to reset my password so that I can regain access to my account"

‚Üì LLM Processing ‚Üì

Generated Test Cases:
1. Test Case: Successful Password Reset
   - Given: User is on forgot password page
   - When: User enters valid email
   - Then: Reset link is sent to email

2. Test Case: Invalid Email Format
   - Given: User is on forgot password page
   - When: User enters invalid email
   - Then: Error message is displayed
```

---

## ‚è±Ô∏è Weekend Timeline (14 Hours)

### **Saturday (7 hours)**

- **Phase 1:** Problem Definition & Planning (30 min)
- **Phase 2:** Data Collection & Preparation (1.5 hours)
- **Phase 3:** Model Selection & Setup (1 hour)
- **Phase 4:** Prompt Engineering (2 hours)
- **Phase 6:** Evaluation & Testing (2 hours)

### **Sunday (7 hours)**

- **Phase 7:** Deployment & Integration (3 hours)
- **Phase 8:** Monitoring & Observability (2 hours)
- **Phase 9:** Testing & Documentation (2 hours)

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              User Story ‚Üí Test Cases System                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[FastAPI Endpoint]
      ‚Üì
[Input Validation]
      ‚Üì
[Prompt Template Engine]
      ‚Üì
[LLM (Ollama/OpenAI)]
      ‚Üì
[Output Parser & Validator]
      ‚Üì
[Quality Metrics (MLflow)]
      ‚Üì
[Response + Metrics Dashboard]
```

---

## üìÇ Project Structure

```
test-case-generator/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ examples/              # Few-shot examples
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user_stories.json
‚îÇ   ‚îî‚îÄ‚îÄ validation/            # Evaluation dataset
‚îÇ       ‚îî‚îÄ‚îÄ test_dataset.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py           # FastAPI app
‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ client.py         # LLM client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompts.py        # Prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ validators/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ structure.py      # Structure validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ quality.py        # Quality metrics
‚îÇ   ‚îî‚îÄ‚îÄ mlflow_tracker.py     # MLflow integration
‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îî‚îÄ‚îÄ test_case_generation.txt
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_api.py
‚îú‚îÄ‚îÄ mlruns/                    # MLflow tracking
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

---

# Phase-by-Phase Implementation

---

## Phase 1: Problem Definition & Planning ‚è±Ô∏è 30 minutes

### **Saturday 9:00 AM - 9:30 AM**

**Objectives:**

- Define success criteria
- Document requirements
- Create decision matrix

**Tasks:**

1. **Create requirements document:**

```yaml
# requirements.yaml
project_name: "Test Case Generator from User Stories"

use_case: |
  Convert user stories (As a...I want...So that...) into structured 
  test cases with Given-When-Then format

success_metrics:
  - structural_validity: ">= 95%" # All test cases have G-W-T structure
  - coverage: ">= 3" # Generate at least 3 test cases per story
  - semantic_relevance: ">= 0.75" # LLM-judge score
  - api_latency: "< 5 seconds" # Response time
  - cost_per_request: "< $0.05" # API cost

constraints:
  - Must work with both Ollama (free) and OpenAI
  - Output must be parseable JSON
  - Must validate quality before returning
  - Must track all metrics in MLflow

deliverables:
  - FastAPI with /generate-test-cases endpoint
  - MLflow dashboard tracking experiments
  - Validation report for each generation
  - Documentation with examples
```

2. **Create decision matrix:**

```python
# docs/decisions.md
# Architecture Decisions

## Model Selection
- **Primary:** Ollama Llama 3.2 3B (free, local)
- **Fallback:** OpenAI GPT-3.5-turbo (higher quality)
- **Reasoning:** Start free, upgrade if quality insufficient

## Output Format
- **Choice:** JSON with strict schema
- **Reasoning:** Easy to validate, parse, and integrate

## Quality Validation
- **Structural:** Pydantic models (all fields present)
- **Semantic:** LLM-as-judge (relevance to user story)
- **Coverage:** Count of test cases (min 3)
- **Reasoning:** Multi-layered validation catches different issues
```

**Deliverables:**
‚úÖ Requirements.yaml  
‚úÖ Decision documentation

---

## Phase 2: Data Collection & Preparation ‚è±Ô∏è 1.5 hours

### **Saturday 9:30 AM - 11:00 AM**

**Objectives:**

- Create few-shot examples
- Build evaluation dataset
- Set up data versioning

**Tasks:**

### **2A: Create Few-Shot Examples** (45 min)

```json
// data/examples/user_stories.json
{
  "examples": [
    {
      "user_story": "As a registered user, I want to log in with my email and password so that I can access my account",
      "test_cases": [
        {
          "id": "TC_001",
          "title": "Successful login with valid credentials",
          "priority": "high",
          "given": "User is on the login page AND has a registered account",
          "when": "User enters valid email and password AND clicks login button",
          "then": "User is redirected to dashboard AND session is created"
        },
        {
          "id": "TC_002",
          "title": "Login fails with invalid password",
          "priority": "high",
          "given": "User is on the login page",
          "when": "User enters valid email and incorrect password",
          "then": "Error message 'Invalid credentials' is displayed AND user remains on login page"
        },
        {
          "id": "TC_003",
          "title": "Login fails with non-existent email",
          "priority": "medium",
          "given": "User is on the login page",
          "when": "User enters email not in system",
          "then": "Error message 'Account not found' is displayed"
        },
        {
          "id": "TC_004",
          "title": "Empty fields validation",
          "priority": "medium",
          "given": "User is on the login page",
          "when": "User clicks login with empty email or password",
          "then": "Validation error messages are displayed for empty fields"
        }
      ]
    },
    {
      "user_story": "As a customer, I want to add items to my shopping cart so that I can purchase multiple products",
      "test_cases": [
        {
          "id": "TC_001",
          "title": "Add single item to empty cart",
          "priority": "high",
          "given": "User is viewing a product page AND cart is empty",
          "when": "User clicks 'Add to Cart' button",
          "then": "Item is added to cart AND cart count shows 1 AND success message appears"
        },
        {
          "id": "TC_002",
          "title": "Add multiple quantities of same item",
          "priority": "high",
          "given": "User is viewing a product page",
          "when": "User selects quantity 3 and clicks 'Add to Cart'",
          "then": "Cart shows 3 units of the product AND total price is calculated correctly"
        },
        {
          "id": "TC_003",
          "title": "Add item when out of stock",
          "priority": "high",
          "given": "Product is marked as out of stock",
          "when": "User attempts to add item to cart",
          "then": "'Add to Cart' button is disabled AND message 'Out of Stock' is displayed"
        }
      ]
    },
    {
      "user_story": "As an admin, I want to delete user accounts so that I can remove inactive or banned users",
      "test_cases": [
        {
          "id": "TC_001",
          "title": "Successful account deletion",
          "priority": "critical",
          "given": "Admin is logged in AND viewing user management page",
          "when": "Admin clicks delete on a user account AND confirms deletion",
          "then": "User account is removed from database AND confirmation message appears"
        },
        {
          "id": "TC_002",
          "title": "Cancel deletion operation",
          "priority": "medium",
          "given": "Admin initiates account deletion",
          "when": "Admin clicks 'Cancel' on confirmation dialog",
          "then": "Account remains unchanged AND user is returned to management page"
        },
        {
          "id": "TC_003",
          "title": "Cannot delete own account",
          "priority": "high",
          "given": "Admin is viewing their own account",
          "when": "Admin attempts to delete their own account",
          "then": "Delete button is disabled OR error message prevents deletion"
        }
      ]
    }
  ]
}
```

### **2B: Create Evaluation Dataset** (45 min)

```json
// data/validation/test_dataset.json
[
  {
    "id": "eval_001",
    "user_story": "As a user, I want to reset my password so that I can regain access to my account",
    "expected_test_count": 4,
    "expected_priorities": ["high", "medium"],
    "must_cover": [
      "successful password reset",
      "invalid email",
      "expired reset link",
      "password requirements"
    ]
  },
  {
    "id": "eval_002",
    "user_story": "As a seller, I want to upload product images so that customers can see what I'm selling",
    "expected_test_count": 5,
    "must_cover": [
      "successful upload",
      "invalid file format",
      "file size limit",
      "multiple images"
    ]
  },
  {
    "id": "eval_003",
    "user_story": "As a user, I want to filter search results by price range so that I can find products within my budget",
    "expected_test_count": 4,
    "must_cover": [
      "filter with valid range",
      "no results in range",
      "invalid price input"
    ]
  },
  {
    "id": "eval_004",
    "user_story": "As a premium member, I want to download invoices so that I can keep records for accounting",
    "expected_test_count": 3,
    "must_cover": [
      "successful download",
      "non-premium user blocked",
      "invoice not found"
    ]
  },
  {
    "id": "eval_005",
    "user_story": "As a moderator, I want to flag inappropriate content so that I can maintain community standards",
    "expected_test_count": 4,
    "must_cover": ["flag with reason", "duplicate flagging", "unflag content"]
  }
]
```

**Deliverables:**
‚úÖ 3 few-shot examples with 12+ test cases  
‚úÖ 5 evaluation user stories

---

## Phase 3: Model Selection & Setup ‚è±Ô∏è 1 hour

### **Saturday 11:00 AM - 12:00 PM**

**Objectives:**

- Install and configure Ollama
- Set up LLM client
- Test model

**Tasks:**

### **3A: Install Ollama** (15 min)

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Start service
ollama serve &

# Pull model
ollama pull llama3.2:3b
```

### **3B: Create LLM Client** (45 min)

```python
# src/llm/client.py
from typing import Optional, Dict, Any
import ollama
import openai
from pydantic import BaseModel
import time
import os

class LLMConfig(BaseModel):
    provider: str = "ollama"
    model: str = "llama3.2:3b"
    temperature: float = 0.3  # Low for consistency
    max_tokens: int = 2000
    api_key: Optional[str] = None

class LLMClient:
    def __init__(self, config: LLMConfig):
        self.config = config
        if config.provider == "openai":
            openai.api_key = config.api_key or os.getenv("OPENAI_API_KEY")

    def generate(self, prompt: str, system_prompt: str = "") -> Dict[str, Any]:
        """Generate response from LLM"""
        start = time.time()

        try:
            if self.config.provider == "ollama":
                response = self._call_ollama(prompt, system_prompt)
            elif self.config.provider == "openai":
                response = self._call_openai(prompt, system_prompt)
            else:
                raise ValueError(f"Unknown provider: {self.config.provider}")

            return {
                "text": response["text"],
                "latency": time.time() - start,
                "tokens": response.get("tokens", 0),
                "model": self.config.model,
                "provider": self.config.provider
            }
        except Exception as e:
            return {
                "text": "",
                "error": str(e),
                "latency": time.time() - start,
                "tokens": 0
            }

    def _call_ollama(self, prompt: str, system_prompt: str) -> dict:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = ollama.chat(
            model=self.config.model,
            messages=messages,
            options={
                "temperature": self.config.temperature,
                "num_predict": self.config.max_tokens
            }
        )

        return {
            "text": response['message']['content'],
            "tokens": response.get('eval_count', 0) + response.get('prompt_eval_count', 0)
        }

    def _call_openai(self, prompt: str, system_prompt: str) -> dict:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        response = openai.chat.completions.create(
            model=self.config.model,
            messages=messages,
            temperature=self.config.temperature,
            max_tokens=self.config.max_tokens
        )

        return {
            "text": response.choices[0].message.content,
            "tokens": response.usage.total_tokens
        }

# Test script
if __name__ == "__main__":
    config = LLMConfig()
    client = LLMClient(config)

    result = client.generate("Say hello")
    print(f"‚úÖ Model test: {result['text']}")
    print(f"Latency: {result['latency']:.2f}s")
```

**Test:**

```bash
python src/llm/client.py
```

**Deliverables:**
‚úÖ Ollama installed and running  
‚úÖ LLM client supporting Ollama + OpenAI  
‚úÖ Successful test generation

---

## Phase 4: Prompt Engineering & Optimization ‚è±Ô∏è 2 hours

### **Saturday 12:00 PM - 2:00 PM**

**Objectives:**

- Design prompt template
- Test and refine
- Version prompts

**Tasks:**

### **4A: Create Prompt Template** (1 hour)

```python
# src/llm/prompts.py
from jinja2 import Template
import json

class PromptBuilder:
    def __init__(self):
        self.system_prompt = """You are an expert QA engineer who creates comprehensive test cases from user stories.

Your task is to generate structured test cases in Given-When-Then format.

RULES:
1. Generate 3-6 test cases covering happy path, edge cases, and error scenarios
2. Each test case MUST have: id, title, priority, given, when, then
3. Priority must be one of: critical, high, medium, low
4. Be specific and actionable in each step
5. Cover positive AND negative scenarios

OUTPUT FORMAT - Respond with ONLY valid JSON (no markdown, no explanations):
{
  "test_cases": [
    {
      "id": "TC_001",
      "title": "Brief descriptive title",
      "priority": "high",
      "given": "Preconditions",
      "when": "Action taken",
      "then": "Expected result"
    }
  ]
}"""

        self.user_template = Template("""
User Story:
{{ user_story }}

{% if examples %}
Examples of good test cases:

{% for example in examples %}
User Story: {{ example.user_story }}
Test Cases Generated:
{{ example.test_cases | tojson(indent=2) }}

{% endfor %}
{% endif %}

Now generate test cases for the user story above. Remember: output ONLY valid JSON.
""")

    def build(self, user_story: str, include_examples: bool = True) -> dict:
        """Build complete prompt"""

        examples = []
        if include_examples:
            examples = self._load_examples()[:2]  # Use 2 examples

        user_prompt = self.user_template.render(
            user_story=user_story,
            examples=examples
        )

        return {
            "system": self.system_prompt,
            "user": user_prompt
        }

    def _load_examples(self):
        """Load few-shot examples"""
        try:
            with open('data/examples/user_stories.json') as f:
                data = json.load(f)
                return data['examples']
        except:
            return []
```

### **4B: Test Prompts** (1 hour)

````python
# tests/test_prompts.py
from src.llm.client import LLMClient, LLMConfig
from src.llm.prompts import PromptBuilder
import json

def test_prompt_quality():
    """Test prompt on sample user stories"""

    client = LLMClient(LLMConfig())
    builder = PromptBuilder()

    test_stories = [
        "As a user, I want to reset my password so that I can regain access",
        "As an admin, I want to export user data so that I can analyze trends",
        "As a customer, I want to track my order so that I know when it arrives"
    ]

    results = []

    for story in test_stories:
        print(f"\n{'='*60}")
        print(f"Testing: {story}")
        print(f"{'='*60}")

        prompts = builder.build(story)
        response = client.generate(
            prompts['user'],
            prompts['system']
        )

        print(f"\nLatency: {response['latency']:.2f}s")
        print(f"Tokens: {response['tokens']}")

        # Try to parse JSON
        try:
            output_text = response['text'].strip()

            # Extract JSON if wrapped in markdown
            if '```json' in output_text:
                output_text = output_text.split('```json')[1].split('```')[0]
            elif '```' in output_text:
                output_text = output_text.split('```')[1].split('```')[0]

            test_cases = json.loads(output_text)

            print(f"‚úÖ Valid JSON")
            print(f"Test cases generated: {len(test_cases.get('test_cases', []))}")

            # Display first test case
            if test_cases.get('test_cases'):
                tc = test_cases['test_cases'][0]
                print(f"\nSample test case:")
                print(f"  Title: {tc.get('title')}")
                print(f"  Priority: {tc.get('priority')}")
                print(f"  Given: {tc.get('given')}")

            results.append({
                "story": story,
                "success": True,
                "count": len(test_cases.get('test_cases', []))
            })

        except json.JSONDecodeError as e:
            print(f"‚ùå JSON Parse Error: {e}")
            print(f"Raw output: {response['text'][:200]}")
            results.append({
                "story": story,
                "success": False,
                "error": str(e)
            })

    # Summary
    print(f"\n{'='*60}")
    print("SUMMARY")
    print(f"{'='*60}")
    successful = sum(1 for r in results if r.get('success'))
    print(f"Success rate: {successful}/{len(results)}")

    return results

if __name__ == "__main__":
    test_prompt_quality()
````

**Run tests:**

```bash
python tests/test_prompts.py
```

**Deliverables:**
‚úÖ Prompt template with few-shot examples  
‚úÖ Tested on 3+ user stories  
‚úÖ ‚â•80% JSON parse success rate

---

## Phase 6: Evaluation & Testing ‚è±Ô∏è 2 hours

### **Saturday 2:00 PM - 4:00 PM**

**Objectives:**

- Implement validators
- Set up MLflow tracking
- Run baseline evaluation

**Tasks:**

### **6A: Structure Validators** (45 min)

```python
# src/validators/structure.py
from pydantic import BaseModel, Field, validator
from typing import List, Literal

class TestCase(BaseModel):
    """Single test case structure"""
    id: str = Field(..., pattern=r"^TC_\d+$")
    title: str = Field(..., min_length=10, max_length=200)
    priority: Literal["critical", "high", "medium", "low"]
    given: str = Field(..., min_length=10)
    when: str = Field(..., min_length=10)
    then: str = Field(..., min_length=10)

    @validator('given', 'when', 'then')
    def not_empty(cls, v):
        if not v or v.strip() == "":
            raise ValueError("Field cannot be empty")
        return v.strip()

class TestCaseOutput(BaseModel):
    """Complete output structure"""
    test_cases: List[TestCase] = Field(..., min_items=3, max_items=10)

    @validator('test_cases')
    def validate_ids_unique(cls, v):
        ids = [tc.id for tc in v]
        if len(ids) != len(set(ids)):
            raise ValueError("Test case IDs must be unique")
        return v

class StructureValidator:
    """Validate test case structure"""

    @staticmethod
    def validate(output_json: dict) -> dict:
        """
        Returns:
        {
            "valid": bool,
            "errors": list,
            "test_cases": list (if valid)
        }
        """
        try:
            validated = TestCaseOutput(**output_json)
            return {
                "valid": True,
                "errors": [],
                "test_cases": [tc.dict() for tc in validated.test_cases],
                "count": len(validated.test_cases)
            }
        except Exception as e:
            return {
                "valid": False,
                "errors": [str(e)],
                "test_cases": [],
                "count": 0
            }
```

### **6B: Quality Metrics** (45 min)

````python
# src/validators/quality.py
from src.llm.client import LLMClient, LLMConfig
import json

class QualityValidator:
    """Evaluate test case quality"""

    def __init__(self, llm_client: LLMClient):
        self.llm_client = llm_client

    def evaluate_relevance(self, user_story: str, test_cases: list) -> dict:
        """Use LLM-as-judge to score relevance"""

        judge_prompt = f"""You are evaluating test cases for quality.

User Story:
{user_story}

Generated Test Cases:
{json.dumps(test_cases, indent=2)}

Rate these test cases on:
1. Relevance to user story (0-10)
2. Coverage of scenarios (0-10)
3. Clarity of steps (0-10)

Respond with ONLY this JSON format:
{{
  "relevance_score": <0-10>,
  "coverage_score": <0-10>,
  "clarity_score": <0-10>,
  "reasoning": "<brief explanation>"
}}
"""

        result = self.llm_client.generate(judge_prompt)

        try:
            # Parse response
            text = result['text'].strip()
            if '```json' in text:
                text = text.split('```json')[1].split('```')[0]
            elif '```' in text:
                text = text.split('```')[1].split('```')[0]

            scores = json.loads(text)

            # Normalize to 0-1
            return {
                "relevance": scores.get('relevance_score', 0) / 10,
                "coverage": scores.get('coverage_score', 0) / 10,
                "clarity": scores.get('clarity_score', 0) / 10,
                "overall": (
                    scores.get('relevance_score', 0) +
                    scores.get('coverage_score', 0) +
                    scores.get('clarity_score', 0)
                ) / 30,
                "reasoning": scores.get('reasoning', ''),
                "passed": (scores.get('relevance_score', 0) / 10) >= 0.7
            }
        except:
            return {
                "relevance": 0.0,
                "coverage": 0.0,
                "clarity": 0.0,
                "overall": 0.0,
                "reasoning": "Failed to parse judge response",
                "passed": False
            }

    def evaluate_coverage(self, test_cases: list, min_count: int = 3) -> dict:
        """Check test case count and diversity"""

        count = len(test_cases)
        priorities = set(tc.get('priority') for tc in test_cases)

        # Check for positive and negative scenarios
        titles_lower = [tc.get('title', '').lower() for tc in test_cases]
        has_positive = any('successful' in t or 'valid' in t for t in titles_lower)
        has_negative = any('fail' in t or 'invalid' in t or 'error' in t for t in titles_lower)

        coverage_score = 0.0
        if count >= min_count:
            coverage_score += 0.4
        if len(priorities) >= 2:
            coverage_score += 0.3
        if has_positive:
            coverage_score += 0.15
        if has_negative:
            coverage_score += 0.15

        return {
            "count": count,
            "min_count_met": count >= min_count,
            "priority_diversity": len(priorities),
            "has_positive_cases": has_positive,
            "has_negative_cases": has_negative,
            "coverage_score": coverage_score,
            "passed": coverage_score >= 0.7
        }
````

### **6C: MLflow Integration** (30 min)

```python
# src/mlflow_tracker.py
import mlflow
import mlflow.pyfunc
from datetime import datetime
import json

class MLflowTracker:
    """Track experiments and metrics in MLflow"""

    def __init__(self, experiment_name: str = "test-case-generation"):
        mlflow.set_experiment(experiment_name)

    def log_generation(
        self,
        user_story: str,
        test_cases: list,
        structure_validation: dict,
        quality_metrics: dict,
        coverage_metrics: dict,
        latency: float,
        model_info: dict
    ):
        """Log a single test case generation run"""

        with mlflow.start_run():
            # Log parameters
            mlflow.log_param("model", model_info.get('model'))
            mlflow.log_param("provider", model_info.get('provider'))
            mlflow.log_param("user_story", user_story[:100])

            # Log metrics
            mlflow.log_metric("latency", latency)
            mlflow.log_metric("structure_valid", 1.0 if structure_validation['valid'] else 0.0)
            mlflow.log_metric("test_case_count", structure_validation['count'])

            if quality_metrics:
                mlflow.log_metric("relevance_score", quality_metrics['relevance'])
                mlflow.log_metric("coverage_score", quality_metrics['coverage'])
                mlflow.log_metric("clarity_score", quality_metrics['clarity'])
                mlflow.log_metric("overall_quality", quality_metrics['overall'])

            if coverage_metrics:
                mlflow.log_metric("coverage_score", coverage_metrics['coverage_score'])
                mlflow.log_metric("priority_diversity", coverage_metrics['priority_diversity'])

            # Log artifacts
            mlflow.log_dict({
                "user_story": user_story,
                "test_cases": test_cases,
                "validations": {
                    "structure": structure_validation,
                    "quality": quality_metrics,
                    "coverage": coverage_metrics
                }
            }, "generation_result.json")

            # Log tags
            mlflow.set_tag("timestamp", datetime.now().isoformat())
            mlflow.set_tag("passed_validation", structure_validation['valid'])

            run_id = mlflow.active_run().info.run_id

        return run_id
```

**Deliverables:**
‚úÖ Structure validation with Pydantic  
‚úÖ Quality metrics (LLM-judge + coverage)  
‚úÖ MLflow tracking configured

---

## Phase 7: Deployment & Integration ‚è±Ô∏è 3 hours

### **Sunday 9:00 AM - 12:00 PM**

**Objectives:**

- Build FastAPI application
- Create endpoints
- Add error handling

**Tasks:**

### **7A: FastAPI Application** (2 hours)

````python
# src/api/main.py
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import Optional, List
import json
from src.llm.client import LLMClient, LLMConfig
from src.llm.prompts import PromptBuilder
from src.validators.structure import StructureValidator
from src.validators.quality import QualityValidator
from src.mlflow_tracker import MLflowTracker
import uvicorn

app = FastAPI(
    title="Test Case Generator API",
    description="Generate structured test cases from user stories",
    version="1.0.0"
)

# Initialize components
llm_config = LLMConfig()
llm_client = LLMClient(llm_config)
prompt_builder = PromptBuilder()
structure_validator = StructureValidator()
quality_validator = QualityValidator(llm_client)
mlflow_tracker = MLflowTracker()

# Request/Response models
class GenerateRequest(BaseModel):
    user_story: str = Field(..., min_length=20, max_length=500)
    include_quality_check: bool = True
    model: Optional[str] = None

class TestCaseResponse(BaseModel):
    id: str
    title: str
    priority: str
    given: str
    when: str
    then: str

class GenerateResponse(BaseModel):
    user_story: str
    test_cases: List[TestCaseResponse]
    validation: dict
    quality_metrics: Optional[dict] = None
    metadata: dict

@app.get("/")
async def root():
    return {
        "service": "Test Case Generator",
        "version": "1.0.0",
        "endpoints": {
            "generate": "/generate-test-cases",
            "health": "/health",
            "metrics": "/metrics"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        # Test LLM connection
        result = llm_client.generate("test", "")
        llm_healthy = "error" not in result

        return {
            "status": "healthy" if llm_healthy else "degraded",
            "llm": "connected" if llm_healthy else "error",
            "model": llm_config.model
        }
    except Exception as e:
        raise HTTPException(status_code=503, detail=f"Service unhealthy: {str(e)}")

@app.post("/generate-test-cases", response_model=GenerateResponse)
async def generate_test_cases(
    request: GenerateRequest,
    background_tasks: BackgroundTasks
):
    """Generate test cases from user story"""

    # Build prompt
    prompts = prompt_builder.build(request.user_story)

    # Generate with LLM
    llm_result = llm_client.generate(
        prompts['user'],
        prompts['system']
    )

    if "error" in llm_result:
        raise HTTPException(
            status_code=500,
            detail=f"LLM generation failed: {llm_result['error']}"
        )

    # Parse JSON output
    try:
        output_text = llm_result['text'].strip()

        # Extract JSON if wrapped
        if '```json' in output_text:
            output_text = output_text.split('```json')[1].split('```')[0]
        elif '```' in output_text:
            output_text = output_text.split('```')[1].split('```')[0]

        output_json = json.loads(output_text)

    except json.JSONDecodeError as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to parse LLM output as JSON: {str(e)}"
        )

    # Validate structure
    structure_validation = structure_validator.validate(output_json)

    if not structure_validation['valid']:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid test case structure: {structure_validation['errors']}"
        )

    test_cases = structure_validation['test_cases']

    # Quality validation (optional, can be slow)
    quality_metrics = None
    if request.include_quality_check:
        quality_metrics = quality_validator.evaluate_relevance(
            request.user_story,
            test_cases
        )

        coverage_metrics = quality_validator.evaluate_coverage(test_cases)
    else:
        coverage_metrics = {"passed": True}

    # Log to MLflow in background
    background_tasks.add_task(
        mlflow_tracker.log_generation,
        user_story=request.user_story,
        test_cases=test_cases,
        structure_validation=structure_validation,
        quality_metrics=quality_metrics or {},
        coverage_metrics=coverage_metrics,
        latency=llm_result['latency'],
        model_info={
            "model": llm_result['model'],
            "provider": llm_result['provider']
        }
    )

    return GenerateResponse(
        user_story=request.user_story,
        test_cases=[TestCaseResponse(**tc) for tc in test_cases],
        validation={
            "structure_valid": structure_validation['valid'],
            "count": structure_validation['count'],
            "quality_passed": quality_metrics['passed'] if quality_metrics else None,
            "coverage_passed": coverage_metrics['passed']
        },
        quality_metrics=quality_metrics,
        metadata={
            "latency": llm_result['latency'],
            "tokens": llm_result['tokens'],
            "model": llm_result['model']
        }
    )

@app.get("/metrics")
async def get_metrics():
    """Get aggregated metrics from MLflow"""
    # This would query MLflow for statistics
    return {
        "message": "View detailed metrics in MLflow UI at http://localhost:5000"
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
````

### **7B: Requirements File** (15 min)

```txt
# requirements.txt
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0
ollama==0.1.6
openai==1.3.0
mlflow==2.9.0
jinja2==3.1.2
python-dotenv==1.0.0
pytest==7.4.3
requests==2.31.0
```

### **7C: Docker Compose** (45 min)

```yaml
# docker-compose.yml
version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-service
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - test-gen-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 30s
      timeout: 10s
      retries: 3

  api:
    build: .
    container_name: test-case-api
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_HOST=http://ollama:11434
    volumes:
      - ./:/app
      - ./mlruns:/app/mlruns
    networks:
      - test-gen-network
    depends_on:
      - ollama
    command: uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload

  mlflow:
    image: python:3.10-slim
    container_name: mlflow-ui
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlruns
    networks:
      - test-gen-network
    command: >
      bash -c "pip install mlflow &&
               mlflow ui --host 0.0.0.0 --port 5000 --backend-store-uri /mlruns"

networks:
  test-gen-network:
    driver: bridge

volumes:
  ollama-data:
```

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Test API:**

```bash
# Start services
docker-compose up -d

# Pull Ollama model
docker exec ollama-service ollama pull llama3.2:3b

# Test endpoint
curl -X POST http://localhost:8000/generate-test-cases \
  -H "Content-Type: application/json" \
  -d '{
    "user_story": "As a user, I want to reset my password so that I can regain access to my account",
    "include_quality_check": true
  }'
```

**Deliverables:**
‚úÖ FastAPI with /generate-test-cases endpoint  
‚úÖ Docker Compose setup  
‚úÖ MLflow UI running on :5000  
‚úÖ Successful API test

---

## Phase 8: Monitoring & Observability ‚è±Ô∏è 2 hours

### **Sunday 12:00 PM - 2:00 PM**

**Objectives:**

- View MLflow dashboard
- Create evaluation script
- Run benchmark

**Tasks:**

### **8A: Evaluation Script** (1.5 hours)

````python
# scripts/run_evaluation.py
import json
from src.llm.client import LLMClient, LLMConfig
from src.llm.prompts import PromptBuilder
from src.validators.structure import StructureValidator
from src.validators.quality import QualityValidator
from src.mlflow_tracker import MLflowTracker

def run_evaluation():
    """Run evaluation on test dataset"""

    # Initialize
    client = LLMClient(LLMConfig())
    builder = PromptBuilder()
    struct_validator = StructureValidator()
    quality_validator = QualityValidator(client)
    tracker = MLflowTracker(experiment_name="evaluation")

    # Load test dataset
    with open('data/validation/test_dataset.json') as f:
        test_data = json.load(f)

    results = []

    print(f"\n{'='*60}")
    print("RUNNING EVALUATION")
    print(f"{'='*60}\n")

    for i, test_case in enumerate(test_data, 1):
        print(f"Test {i}/{len(test_data)}: {test_case['id']}")

        # Generate
        prompts = builder.build(test_case['user_story'])
        llm_result = client.generate(prompts['user'], prompts['system'])

        # Parse
        try:
            output_text = llm_result['text'].strip()
            if '```json' in output_text:
                output_text = output_text.split('```json')[1].split('```')[0]
            elif '```' in output_text:
                output_text = output_text.split('```')[1].split('```')[0]

            output_json = json.loads(output_text)
        except Exception as e:
            print(f"  ‚ùå Parse error: {e}")
            results.append({
                "id": test_case['id'],
                "passed": False,
                "error": "parse_error"
            })
            continue

        # Validate
        struct_val = struct_validator.validate(output_json)

        if not struct_val['valid']:
            print(f"  ‚ùå Structure invalid")
            results.append({
                "id": test_case['id'],
                "passed": False,
                "error": "structure_invalid"
            })
            continue

        # Quality check
        quality = quality_validator.evaluate_relevance(
            test_case['user_story'],
            struct_val['test_cases']
        )

        coverage = quality_validator.evaluate_coverage(
            struct_val['test_cases'],
            test_case.get('expected_test_count', 3)
        )

        passed = (
            struct_val['valid'] and
            quality['passed'] and
            coverage['passed']
        )

        # Log to MLflow
        tracker.log_generation(
            user_story=test_case['user_story'],
            test_cases=struct_val['test_cases'],
            structure_validation=struct_val,
            quality_metrics=quality,
            coverage_metrics=coverage,
            latency=llm_result['latency'],
            model_info={
                "model": llm_result['model'],
                "provider": llm_result['provider']
            }
        )

        print(f"  {'‚úÖ' if passed else '‚ùå'} Pass: {passed}")
        print(f"     Quality: {quality['overall']:.2f}")
        print(f"     Coverage: {coverage['coverage_score']:.2f}")

        results.append({
            "id": test_case['id'],
            "passed": passed,
            "count": struct_val['count'],
            "quality": quality['overall'],
            "coverage": coverage['coverage_score'],
            "latency": llm_result['latency']
        })

    # Summary
    print(f"\n{'='*60}")
    print("EVALUATION SUMMARY")
    print(f"{'='*60}")

    total = len(results)
    passed = sum(1 for r in results if r.get('passed'))
    avg_quality = sum(r.get('quality', 0) for r in results) / total
    avg_coverage = sum(r.get('coverage', 0) for r in results) / total
    avg_latency = sum(r.get('latency', 0) for r in results) / total

    print(f"Pass Rate: {passed}/{total} ({passed/total*100:.1f}%)")
    print(f"Avg Quality Score: {avg_quality:.2f}")
    print(f"Avg Coverage Score: {avg_coverage:.2f}")
    print(f"Avg Latency: {avg_latency:.2f}s")

    # Save results
    with open('reports/evaluation_results.json', 'w') as f:
        json.dump({
            "summary": {
                "total": total,
                "passed": passed,
                "pass_rate": passed/total,
                "avg_quality": avg_quality,
                "avg_coverage": avg_coverage,
                "avg_latency": avg_latency
            },
            "results": results
        }, f, indent=2)

    print(f"\n‚úÖ Results saved to reports/evaluation_results.json")
    print(f"üìä View MLflow dashboard: http://localhost:5000\n")

if __name__ == "__main__":
    import os
    os.makedirs('reports', exist_ok=True)
    run_evaluation()
````

**Run evaluation:**

```bash
python scripts/run_evaluation.py
```

**Deliverables:**
‚úÖ Evaluation script  
‚úÖ Results logged to MLflow  
‚úÖ Summary report generated

---

## Phase 9: Testing & Documentation ‚è±Ô∏è 2 hours

### **Sunday 2:00 PM - 4:00 PM**

**Objectives:**

- Write comprehensive README
- Create usage examples
- Final testing

**Tasks:**

### **9A: Documentation** (1 hour)

````markdown
# Test Case Generator - LLMOps Project

Generate structured test cases from user stories using LLMs with quality validation and MLflow tracking.

## Features

‚úÖ **REST API** - FastAPI endpoint for test case generation  
‚úÖ **LLM-Powered** - Uses Ollama (free) or OpenAI  
‚úÖ **Quality Validation** - Structure + semantic validation  
‚úÖ **MLflow Tracking** - Complete experiment tracking  
‚úÖ **Docker Support** - Easy deployment with Docker Compose

## Quick Start

### 1. Setup

```bash
git clone <repo>
cd test-case-generator

# Start services
docker-compose up -d

# Pull Ollama model
docker exec ollama-service ollama pull llama3.2:3b
```
````

### 2. Generate Test Cases

```bash
curl -X POST http://localhost:8000/generate-test-cases \
  -H "Content-Type: application/json" \
  -d '{
    "user_story": "As a user, I want to reset my password so I can regain access",
    "include_quality_check": true
  }'
```

### 3. View Metrics

Open MLflow UI: http://localhost:5000

## API Documentation

### POST /generate-test-cases

**Request:**

```json
{
  "user_story": "As a [role], I want [feature] so that [benefit]",
  "include_quality_check": true
}
```

**Response:**

```json
{
  "user_story": "...",
  "test_cases": [
    {
      "id": "TC_001",
      "title": "Successful password reset",
      "priority": "high",
      "given": "User is on forgot password page",
      "when": "User enters valid email",
      "then": "Reset link is sent"
    }
  ],
  "validation": {
    "structure_valid": true,
    "count": 4,
    "quality_passed": true
  },
  "metadata": {
    "latency": 2.3,
    "tokens": 450,
    "model": "llama3.2:3b"
  }
}
```

## Project Structure

```
test-case-generator/
‚îú‚îÄ‚îÄ data/                  # Training & eval data
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/              # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ llm/              # LLM client & prompts
‚îÇ   ‚îú‚îÄ‚îÄ validators/       # Quality validators
‚îÇ   ‚îî‚îÄ‚îÄ mlflow_tracker.py
‚îú‚îÄ‚îÄ mlruns/               # MLflow experiments
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

## Running Evaluation

```bash
python scripts/run_evaluation.py
```

View results in `reports/evaluation_results.json` and MLflow UI.

## Configuration

Edit model in `src/llm/client.py`:

```python
LLMConfig(
    provider="ollama",  # or "openai"
    model="llama3.2:3b",
    temperature=0.3
)
```

## Metrics Tracked

- **Structure Validation**: All fields present & valid
- **Quality Score**: LLM-judge evaluation (0-1)
- **Coverage**: Test case count & diversity
- **Latency**: Response time
- **Cost**: Token usage

## License

MIT

````

### **9B: Example Notebook** (30 min)

```python
# examples/usage_example.py
import requests
import json

# Example user stories
user_stories = [
    "As a customer, I want to track my order so that I know when it will arrive",
    "As an admin, I want to export user data so that I can analyze trends",
    "As a seller, I want to upload product images so customers can see my items"
]

def generate_test_cases(user_story: str):
    """Generate test cases via API"""

    response = requests.post(
        "http://localhost:8000/generate-test-cases",
        json={
            "user_story": user_story,
            "include_quality_check": True
        }
    )

    if response.status_code == 200:
        return response.json()
    else:
        print(f"Error: {response.status_code}")
        print(response.text)
        return None

# Generate for each story
for story in user_stories:
    print(f"\n{'='*60}")
    print(f"User Story: {story}")
    print(f"{'='*60}\n")

    result = generate_test_cases(story)

    if result:
        print(f"Generated {len(result['test_cases'])} test cases:")
        for tc in result['test_cases'][:2]:  # Show first 2
            print(f"\n  {tc['id']}: {tc['title']}")
            print(f"  Priority: {tc['priority']}")
            print(f"  Given: {tc['given'][:50]}...")

        print(f"\nValidation:")
        print(f"  Structure: {'‚úÖ' if result['validation']['structure_valid'] else '‚ùå'}")
        print(f"  Quality: {'‚úÖ' if result['validation']['quality_passed'] else '‚ùå'}")
        print(f"  Latency: {result['metadata']['latency']:.2f}s")
````

### **9C: Final Testing** (30 min)

```bash
# Test checklist

# 1. Health check
curl http://localhost:8000/health

# 2. Generate test cases
curl -X POST http://localhost:8000/generate-test-cases \
  -H "Content-Type: application/json" \
  -d '{"user_story": "As a user, I want to log in so I can access my account"}'

# 3. Check MLflow
open http://localhost:5000

# 4. Run evaluation
python scripts/run_evaluation.py

# 5. View results
cat reports/evaluation_results.json
```

**Deliverables:**
‚úÖ Complete README  
‚úÖ Usage examples  
‚úÖ All endpoints tested  
‚úÖ MLflow dashboard populated

---

## üìä Success Criteria Checklist

### ‚úÖ **Phase 1: Planning**

- [ ] Requirements documented
- [ ] Success metrics defined

### ‚úÖ **Phase 2: Data**

- [ ] 3 few-shot examples created
- [ ] 5 evaluation user stories prepared

### ‚úÖ **Phase 3: Model**

- [ ] Ollama installed and running
- [ ] LLM client working

### ‚úÖ **Phase 4: Prompts**

- [ ] Prompt template with few-shot examples
- [ ] ‚â•80% JSON parse success

### ‚úÖ **Phase 6: Evaluation**

- [ ] Structure validation (Pydantic)
- [ ] Quality metrics (LLM-judge)
- [ ] MLflow tracking configured

### ‚úÖ **Phase 7: Deployment**

- [ ] FastAPI endpoint working
- [ ] Docker Compose running
- [ ] MLflow UI accessible

### ‚úÖ **Phase 8: Monitoring**

- [ ] Evaluation script runs
- [ ] Metrics logged to MLflow
- [ ] ‚â•70% pass rate on eval set

### ‚úÖ **Phase 9: Documentation**

- [ ] Complete README
- [ ] Usage examples
- [ ] All tests passing

---

## üéØ Final Deliverables

1. **Working API**: http://localhost:8000
2. **MLflow Dashboard**: http://localhost:5000
3. **Evaluation Results**: reports/evaluation_results.json
4. **Documentation**: README.md
5. **GitHub Repo**: Ready for portfolio

---

## üí° Next Steps After Weekend

1. **Enhancements:**
   - Add more validation rules
   - Support different test frameworks (Cucumber, pytest)
   - Add batch processing endpoint
   - Export to JIRA/TestRail

2. **Advanced Features:**
   - Fine-tune model on your test cases
   - Add human-in-the-loop review
   - Create CI/CD pipeline
   - Deploy to cloud (Railway, Fly.io)

3. **Portfolio:**
   - Blog post: "Building an LLM-Powered Test Generator"
   - Demo video
   - LinkedIn post with metrics

---

**Start Saturday morning at 9 AM and you'll have a production-ready LLMOps system by Sunday at 4 PM! üöÄ**
